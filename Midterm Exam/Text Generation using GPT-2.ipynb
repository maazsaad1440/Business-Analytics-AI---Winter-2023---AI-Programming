{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Question 3(b)(i) - Natural Language Processing - Text Generation\n",
        "\n",
        "# Honour Code\n",
        "# I, Maaz Saad, hereby give my word of honour that I am the sole author of the work as submitted in the \n",
        "# answers to this midterm examination. I didn’t collaborate with anyone and didn’t let anybody copy my work. \n",
        "# If I used code written by someone else, I clearly and explicitly acknowledged the source using comments \n",
        "# within the Python code, and I am aware that I will not receive any credits solely for code written by \n",
        "# someone else. Further, I am aware that failing to clearly credit the source/author of the reused code \n",
        "# will be considered academic misconduct. I didn’t post or submit, and will never post or submit, \n",
        "# questions or answers, or parts of questions or answers of this midterm to a third party \n",
        "# (except for my submission in Canvas for this midterm)."
      ],
      "metadata": {
        "id": "Uxk4_wrV3z3O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Source 1: https://huggingface.co/gpt2\n",
        "# Source 2: https://huggingface.co/blog/how-to-generate\n",
        "\n",
        "# Install Libraries\n",
        "! pip install transformers\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed\n",
        "\n",
        "# Load the GPT-2 model and Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Since the generation relies on some randomness, we set a seed for reproducibility\n",
        "set_seed(33)\n",
        "\n",
        "# Generate Some Text\n",
        "Prompt = 'The man helped with the'\n",
        "input_ids = tokenizer.encode(Prompt, return_tensors='pt')\n",
        "output = model.generate(input_ids, max_length=96, num_return_sequences=1,\n",
        "                        num_beams=3, no_repeat_ngram_size=3, early_stopping=True)\n",
        "\n",
        "# Decode the output tokens back into text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the final text\n",
        "print(generated_text)\n",
        "\n",
        "# Genereated text\n",
        "# The man helped with the rest of the work.\n",
        "# \"I'm not going to lie to you,\" he said. \"I'm just going to tell you what happened.\"\n",
        "# The man said he had been in a car with his wife and two children when he was shot. \n",
        "# He said he was in the back seat of the car when he heard the shots. \n",
        "# He told police he heard a loud bang and saw a man with a gun in the passenger seat."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD9JOrMQ0wai",
        "outputId": "7dfee2d0-d39e-4ce5-d146-0bd215dc8042"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The man helped with the rest of the work.\n",
            "\n",
            "\"I'm not going to lie to you,\" he said. \"I'm just going to tell you what happened.\"\n",
            "\n",
            "The man said he had been in a car with his wife and two children when he was shot. He said he was in the back seat of the car when he heard the shots. He told police he heard a loud bang and saw a man with a gun in the passenger seat.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}